name: NBA Data Scrapers

on:
  schedule:
    # Daily stats: 4x per day (8 AM, 12 PM, 6 PM, 11 PM EST = 1 PM, 5 PM, 11 PM, 4 AM UTC)
    - cron: '0 13 * * *'  # 8 AM EST
    - cron: '0 17 * * *'  # 12 PM EST
    - cron: '0 23 * * *'  # 6 PM EST
    - cron: '0 4 * * *'   # 11 PM EST
  workflow_dispatch: # Allow manual trigger

jobs:
  scrape-all:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          cd scraper
          pip install -r requirements.txt
          
      - name: Run daily stats scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          cd scraper
          echo "=== Starting Daily Stats Scraper ==="
          python daily_stats_scraper.py
          echo "=== Daily Stats Scraper Complete ==="
        
      - name: Run value index calculator
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          cd scraper
          echo "=== Starting Value Index Calculator ==="
          python enhanced_value_index.py
          echo "=== Value Index Calculator Complete ==="
        continue-on-error: true

      - name: Run sentiment scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          REDDIT_USER_AGENT: ${{ secrets.REDDIT_USER_AGENT }}
        run: |
          cd scraper
          echo "=== Starting Sentiment Scraper ==="
          python enhanced_sentiment_scraper.py
          echo "=== Sentiment Scraper Complete ==="
        continue-on-error: true
        
      - name: Verify data was updated
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          cd scraper
          python3 << 'EOF'
          import os
          from supabase import create_client
          import datetime
          
          url = os.environ.get("SUPABASE_URL")
          key = os.environ.get("SUPABASE_KEY")
          supabase = create_client(url, key)
          
          # Check for recent stats
          yesterday = (datetime.date.today() - datetime.timedelta(days=1)).isoformat()
          response = supabase.table('daily_player_stats').select('game_date').eq('game_date', yesterday).limit(1).execute()
          
          if response.data:
              print(f"✅ Data verified: Found stats for {yesterday}")
          else:
              print(f"⚠️  No stats found for {yesterday} - this might be normal if no games were played")
          EOF
        continue-on-error: true
        
      - name: Clear API cache
        run: |
          echo "=== Clearing API cache ==="
          curl -X POST https://nba-analytics-api-2sal.onrender.com/admin/clear-cache || echo "Cache clear failed, but continuing..."
          echo "=== Cache clear complete ==="
        continue-on-error: true
        
      - name: Summary
        if: always()
        run: |
          echo "=== Scraper Run Summary ==="
          echo "Check the logs above for any errors"
          echo "If you see 'No games found', that's normal for off days"
          echo "If you see errors, check your GitHub secrets configuration"
